{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54742490",
   "metadata": {},
   "source": [
    "# Spark Streaming and Senzing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85510a2a",
   "metadata": {},
   "source": [
    "This notebook shows you how to process streaming data through Apache Spark and send it to Senzing for entity resolution, simulating a real-time data processing pipeline. If you haven't already gone through the `senzing_quickstart` tutorial in this repository, we recommend starting with that one because it contains more detailed explanations for each of the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70eff95",
   "metadata": {},
   "source": [
    "### Steps in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459273ad",
   "metadata": {},
   "source": [
    "1. Set up the Senzing gRPC server, download the `customer.json` data file and split it into 20 separate JSON files to simulate streaming data.\n",
    "2. Configure the Senzing engine so it's ready to receive data.\n",
    "3. Create a Spark session with streaming capabilities, define a schema and set up a streaming dataframe.\n",
    "4. Implement a batch processing function that takes each streaming batch from Spark, sends individual records to Senzing for entity resolution, and tracks which entities are affected by each record addition.\n",
    "5. Run a cleanup process to ensure the entities are as accurate as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f32414",
   "metadata": {},
   "source": [
    "## Set up requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0d89",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use the [`senzing`](https://garage.senzing.com/sz-sdk-python/index.html) and [`senzing_grpc`](https://garage.senzing.com/sz-sdk-python-grpc/) packages, in addition to PySpark. You can install all of these using the `requirements.txt` file in the repo folder containing this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from senzing import SzEngineFlags, SzError\n",
    "from senzing_grpc import SzAbstractFactoryGrpc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f36227",
   "metadata": {},
   "source": [
    "We'll start our [Senzing gRPC server](https://github.com/senzing-garage/serve-grpc/tree/main) using Docker.\n",
    "\n",
    "Run the following command `docker run -it --publish 8261:8261 --rm senzing/serve-grpc` in a terminal window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40530f4a",
   "metadata": {},
   "source": [
    "Then, we'll download the example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"./data/\"\n",
    "data_url_prefix = \"https://raw.githubusercontent.com/Senzing/truth-sets/refs/heads/main/truthsets/demo/\"\n",
    "filename = \"customers.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "url = data_url_prefix + filename\n",
    "filepath = data_path + filename\n",
    "if not os.path.exists(filepath):\n",
    "    response = requests.get(url, stream=True, timeout=10)\n",
    "    response.raw.decode_content = True\n",
    "    with open(filepath, \"wb\") as file:\n",
    "        shutil.copyfileobj(response.raw, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952a68e",
   "metadata": {},
   "source": [
    "## Create separate json files to simulate streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ae673",
   "metadata": {},
   "source": [
    "We'll use the `customers.json` dataset from the `spark_quickstart` tutorial, but this time we'll split it into separate json files. We'll extract the first 20 records in the `customers.json` file, and we'll save each record into a separate json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa194d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streaming_files(input_file, output_dir, n_rows):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n_rows:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                filename = f\"{output_dir}/record_{record['RECORD_ID']}.json\"\n",
    "                \n",
    "                with open(filename, 'w') as out_file:\n",
    "                    json.dump(record, out_file)\n",
    "                    \n",
    "                print(f\"Created {filename}\")\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {i}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_streaming_files('data/customers.json', 'data/streaming', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e185bd",
   "metadata": {},
   "source": [
    "## Configure Senzing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec216b",
   "metadata": {},
   "source": [
    "Next, we'll configure the Senzing engine to accept the `customers` data, in the same way as the `spark_quickstart` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_channel = grpc.insecure_channel(\"localhost:8261\")\n",
    "sz_abstract_factory = SzAbstractFactoryGrpc(grpc_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d822b",
   "metadata": {},
   "source": [
    "We'll check connectivity by getting the Senzing version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_product = sz_abstract_factory.create_product()\n",
    "print(json.dumps(json.loads(sz_product.get_version()), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_configmanager = sz_abstract_factory.create_configmanager()\n",
    "sz_diagnostic = sz_abstract_factory.create_diagnostic()\n",
    "sz_engine = sz_abstract_factory.create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id = sz_configmanager.get_default_config_id()\n",
    "sz_config = sz_configmanager.create_config_from_config_id(config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64881f",
   "metadata": {},
   "source": [
    "This time, we'll only use a single data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a23e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_config.register_data_source('CUSTOMERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_config = sz_config.export()\n",
    "new_config_id = sz_configmanager.register_config(new_json_config, \"Spark Streaming\")\n",
    "sz_configmanager.replace_default_config_id(config_id, new_config_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_abstract_factory.reinitialize(new_config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca643f26",
   "metadata": {},
   "source": [
    "## Set up Spark streaming functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dccf1b",
   "metadata": {},
   "source": [
    "We'll start a new Spark session, create a schema, and then set up a stream reader from Spark's [Structured Streaming](https://spark.apache.org/docs/latest/streaming/index.html) engine. In the next section, we'll use a stream writer to send the data from the Spark streaming dataframe to Senzing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bddc7b",
   "metadata": {},
   "source": [
    "First, we'll create a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Senzing Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e861b",
   "metadata": {},
   "source": [
    "Providing a schema for our data makes sure that all the files have the correct information, and also speeds up the Spark stream reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a037025",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_schema = StructType([\n",
    "    StructField(\"DATA_SOURCE\", StringType(), True),\n",
    "    StructField(\"RECORD_ID\", StringType(), True),\n",
    "    StructField(\"RECORD_TYPE\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_ORG\", StringType(), True),\n",
    "    StructField(\"SECONDARY_NAME_ORG\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_FULL\", StringType(), True),\n",
    "    StructField(\"NATIVE_NAME_FULL\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_LAST\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_FIRST\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_MIDDLE\", StringType(), True),\n",
    "    StructField(\"GENDER\", StringType(), True),\n",
    "    StructField(\"DATE_OF_BIRTH\", StringType(), True),\n",
    "    StructField(\"PASSPORT_NUMBER\", StringType(), True),\n",
    "    StructField(\"PASSPORT_COUNTRY\", StringType(), True),\n",
    "    StructField(\"DRIVERS_LICENSE_NUMBER\", StringType(), True),\n",
    "    StructField(\"DRIVERS_LICENSE_STATE\", StringType(), True),\n",
    "    StructField(\"SSN_NUMBER\", StringType(), True),\n",
    "    StructField(\"NATIONAL_ID_NUMBER\", StringType(), True),\n",
    "    StructField(\"NATIONAL_ID_COUNTRY\", StringType(), True),\n",
    "    StructField(\"ADDR_TYPE\", StringType(), True),\n",
    "    StructField(\"ADDR_FULL\", StringType(), True),\n",
    "    StructField(\"ADDR_LINE1\", StringType(), True),\n",
    "    StructField(\"ADDR_CITY\", StringType(), True),\n",
    "    StructField(\"ADDR_STATE\", StringType(), True),\n",
    "    StructField(\"ADDR_POSTAL_CODE\", StringType(), True),\n",
    "    StructField(\"ADDR_COUNTRY\", StringType(), True),\n",
    "    StructField(\"PHONE_TYPE\", StringType(), True),\n",
    "    StructField(\"PHONE_NUMBER\", StringType(), True),\n",
    "    StructField(\"EMAIL_ADDRESS\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"STATUS\", StringType(), True),\n",
    "    StructField(\"CATEGORY\", StringType(), True),\n",
    "    StructField(\"AMOUNT\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d24be2",
   "metadata": {},
   "source": [
    "The stream reader uses this schema to write to a streaming dataframe. For this example, it reads one file at a time to simulate streaming, but you can easily change this to your input stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .schema(customers_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1)  \\\n",
    "    .json('data/streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496e7a7",
   "metadata": {},
   "source": [
    "## Add records to Senzing and to Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55026b",
   "metadata": {},
   "source": [
    "We'll use the `get_affected_entities` function from the `spark_quickstart` tutorial to track what entities have been changed or created in the Senzing repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affected_entities(info_string):\n",
    "    # helper function to extract the entity id\n",
    "    info = json.loads(info_string)\n",
    "    return [entity['ENTITY_ID'] for entity in info['AFFECTED_ENTITIES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b512e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "affected_entities = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab17a0",
   "metadata": {},
   "source": [
    "And we'll use the code from the `spark_quickstart` tutorial to create a function that will send a streaming batch to the Senzing engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_streaming_batch(batch_df, batch_id):\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
    "    \n",
    "    for row in batch_df.rdd.toLocalIterator():\n",
    "        record = {k: v for k, v in row.asDict().items() if v is not None}\n",
    "        \n",
    "        info = sz_engine.add_record(\n",
    "            record['DATA_SOURCE'],\n",
    "            record['RECORD_ID'], \n",
    "            record,\n",
    "            SzEngineFlags.SZ_WITH_INFO,\n",
    "        )\n",
    "        \n",
    "        affected_entities.update(get_affected_entities(info))\n",
    "        print(f\"Added record {record['RECORD_ID']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d2506",
   "metadata": {},
   "source": [
    "Then, we'll stream the data from the Spark dataframe to Senzing using a Spark stream writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_streaming_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b65c3f",
   "metadata": {},
   "source": [
    "We can view the `affected_entities` set to confirm that entities have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1342750",
   "metadata": {},
   "outputs": [],
   "source": [
    "affected_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a6667",
   "metadata": {},
   "source": [
    "## Process REDO records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37aa0c8",
   "metadata": {},
   "source": [
    "As in the `spark_quickstart` tutorial, we'll run the Senzing [redo process](https://senzing.zendesk.com/hc/en-us/articles/360007475133-Processing-REDO) to clean up the entities in the Senzing repository, updating the `affected_entities` set as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    redo_record = sz_engine.get_redo_record()\n",
    "    if not redo_record:\n",
    "        break\n",
    "    info = sz_engine.process_redo_record(redo_record, flags=SzEngineFlags.SZ_WITH_INFO)\n",
    "    affected_entities.update(get_affected_entities(info))\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3fd",
   "metadata": {},
   "source": [
    "## Show dashboard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check things were added to Senzing!\n",
    "\n",
    "search_query = {\n",
    "    \"name_full\": \"robert smith\",\n",
    "    \"date_of_birth\": \"11/12/1978\",\n",
    "}\n",
    "search_result = sz_engine.search_by_attributes(json.dumps(search_query))\n",
    "print(json.dumps(json.loads(search_result), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4eaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senzing-databricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
